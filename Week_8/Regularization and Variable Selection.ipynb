{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Variable Selection in linear models (Shrinkage Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO, Ridge and Elastic Net\n",
    "\n",
    "\n",
    "### Ridge\n",
    "Ridge uses a tunable parameter that is used to minimized the coefficients as low as possible using this equation.\n",
    "\n",
    "RSS(residual  sum  of  squares) $+ \\lambda \\sum_{j=1}^{p} \\beta_j^2$\n",
    "\n",
    "$\\lambda  is \\leq 0$ In this case ridge coefficients are adjusted to fit the data well but there is no really reduction of coefficients unless there contribution to the model is almost 0. Similar to the least squares technique for linear model ridge tries to minimize the RSS but it adds and adittional paramenter called the shrinkage penalty and this tends to reduce the coefficients toward zero.\n",
    "\n",
    "### LASSO\n",
    "LASSO relies open the linear model to reduce features using an alternative fitting procedure for estimating the coefficients. This procedure is more restrictive and reduce the coefficients that do not contribute to the model to 0.\n",
    "\n",
    "It also uses a tunable parameter $\\lambda$\n",
    "\n",
    "$$RSS + \\lambda \\sum_{j=1}^{p} |\\beta_j|$$\n",
    "\n",
    "LASSO uses an L1 penalty instead of an L2. as mentioned before the shrinkage penalty in lasso tends to recude some variables to zero, if they have very low coefficients to beging with.\n",
    "\n",
    "### Elastic Net\n",
    "\n",
    "As we will explore next, LASSO present problems whit highly correlated datasets or high dimensional data where LASSO tends to select one variable from a group and ignore the others. Elastic net overcomes this difficulties by addind a quadratic term to the shrinkage penalty \n",
    "\n",
    "$$RSS + \\lambda \\sum_{j=1}^{p} |\\beta_j|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Bias_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the tutorial from [https://www4.stat.ncsu.edu/~post/josh/LASSO_Ridge_Elastic_Net_-_Examples.html](https://www4.stat.ncsu.edu/~post/josh/LASSO_Ridge_Elastic_Net_-_Examples.html). That gives a very good explanation to the differences and strenghts to the three different methods for variable selection.\n",
    "\n",
    "In the first section we will simulate data taken from a normal distribution, where there is very small signal and lots of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)  # Package needed to generate correlated precictors\n",
    "library(glmnet)  # Package to fit ridge/lasso/elastic net models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "set.seed(19875)  # Set seed for reproducibility\n",
    "n <- 1000  # Number of observations\n",
    "p <- 5000  # Number of predictors included in model\n",
    "real_p <- 15  # Number of true predictors\n",
    "x <- matrix(rnorm(n*p), nrow=n, ncol=p)\n",
    "y <- apply(x[,1:real_p], 1, sum) + rnorm(n)\n",
    "\n",
    "# Split data into train (2/3) and test (1/3) sets\n",
    "train_rows <- sample(1:n, .66*n)\n",
    "x.train <- x[train_rows, ]\n",
    "x.test <- x[-train_rows, ]\n",
    "\n",
    "y.train <- y[train_rows]\n",
    "y.test <- y[-train_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models \n",
    "# (For plots on left):\n",
    "fit.lasso <- glmnet(x.train, y.train, family=\"gaussian\", alpha=1)\n",
    "fit.ridge <- glmnet(x.train, y.train, family=\"gaussian\", alpha=0)\n",
    "fit.elnet <- glmnet(x.train, y.train, family=\"gaussian\", alpha=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0\n",
    "# (For plots on Right)\n",
    "for (i in 0:10) {\n",
    "    assign(paste(\"fit\", i, sep=\"\"), cv.glmnet(x.train, y.train, type.measure=\"mse\", \n",
    "                                              alpha=i/10,family=\"gaussian\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot solution paths:\n",
    "par(mfrow=c(3,2))\n",
    "# For plotting options, type '?plot.glmnet' in R console\n",
    "plot(fit.lasso, xvar=\"lambda\",label=TRUE)\n",
    "plot(fit10, main=\"LASSO\")\n",
    "\n",
    "plot(fit.ridge, xvar=\"lambda\")\n",
    "plot(fit0, main=\"Ridge\")\n",
    "\n",
    "plot(fit.elnet, xvar=\"lambda\")\n",
    "plot(fit5, main=\"Elastic Net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE on test set\n",
    "yhat0 <- predict(fit0, s=fit0$lambda.1se, newx=x.test)\n",
    "yhat1 <- predict(fit1, s=fit1$lambda.1se, newx=x.test)\n",
    "yhat2 <- predict(fit2, s=fit2$lambda.1se, newx=x.test)\n",
    "yhat3 <- predict(fit3, s=fit3$lambda.1se, newx=x.test)\n",
    "yhat4 <- predict(fit4, s=fit4$lambda.1se, newx=x.test)\n",
    "yhat5 <- predict(fit5, s=fit5$lambda.1se, newx=x.test)\n",
    "yhat6 <- predict(fit6, s=fit6$lambda.1se, newx=x.test)\n",
    "yhat7 <- predict(fit7, s=fit7$lambda.1se, newx=x.test)\n",
    "yhat8 <- predict(fit8, s=fit8$lambda.1se, newx=x.test)\n",
    "yhat9 <- predict(fit9, s=fit9$lambda.1se, newx=x.test)\n",
    "yhat10 <- predict(fit10, s=fit10$lambda.1se, newx=x.test)\n",
    "\n",
    "mse0 <- mean((y.test - yhat0)^2)\n",
    "mse1 <- mean((y.test - yhat1)^2)\n",
    "mse2 <- mean((y.test - yhat2)^2)\n",
    "mse3 <- mean((y.test - yhat3)^2)\n",
    "mse4 <- mean((y.test - yhat4)^2)\n",
    "mse5 <- mean((y.test - yhat5)^2)\n",
    "mse6 <- mean((y.test - yhat6)^2)\n",
    "mse7 <- mean((y.test - yhat7)^2)\n",
    "mse8 <- mean((y.test - yhat8)^2)\n",
    "mse9 <- mean((y.test - yhat9)^2)\n",
    "mse10 <- mean((y.test - yhat10)^2)\n",
    "\n",
    "mse0 ##Ridge\n",
    "mse2\n",
    "mse4\n",
    "mse8\n",
    "mse10 ##Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_cv_Ridge <- min(fit.ridge$lambda)\n",
    "lambda_cv_Lasso <- min(fit.lasso$lambda)\n",
    "lambda_cv_Elnet <- min(fit.elnet$lambda)\n",
    "lambda_cv_Ridge\n",
    "lambda_cv_Lasso\n",
    "lambda_cv_Elnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIC is an out of sample estimator that can be used to evaluate the quality of the model. low values of AIC equate better models. BIC is implemented similarly than AIC but it differs in the penalty given from the number of paramenters where AIC uses 2k (where k is the number of paramenters from the model) and BIC uses a penaly of ln(n)k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](AIC.png)\n",
    "\n",
    "More information at [https://towardsdatascience.com/introduction-to-aic-akaike-information-criterion-9c9ba1c96ced](https://towardsdatascience.com/introduction-to-aic-akaike-information-criterion-9c9ba1c96ced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Find best AIC/BIC for each model\n",
    "##Ridge\n",
    "tLL <- fit.ridge$nulldev - deviance(fit.ridge) ##Likelihood of the model\n",
    "k <- fit.ridge$df ##Number of parameters\n",
    "n <- fit.ridge$nobs ##Sample Size\n",
    "\n",
    "##AIC\n",
    "\n",
    "AICc <- -tLL+2*k+2*k*(k+1)/(n-k-1)\n",
    "min(AICc)##Best AIC\n",
    "best_AIC = match(c(min(AICc)),AICc)##Which index is this?\n",
    "print(paste(\"Lambda from AIC Ridge = \",fit.ridge$lambda[best_AIC]))##Find this lambda\n",
    "\n",
    "##BIC\n",
    "BIC<-log(n)*k - tLL \n",
    "min(BIC)##Best BIC\n",
    "best_BIC = match(c(min(BIC)),BIC)##Which index is this?\n",
    "print(paste(\"Lambda from BIC Ridge = \",fit.ridge$lambda[best_BIC]))\n",
    "\n",
    "##Lasso\n",
    "##AIC\n",
    "tLL <- fit.lasso$nulldev - deviance(fit.lasso)\n",
    "k <- fit.lasso$df\n",
    "n <- fit.lasso$nobs\n",
    "AICc_lasso <- -tLL+2*k+2*k*(k+1)/(n-k-1)\n",
    "min(AICc_lasso)##Best AIC\n",
    "best_AIC_lasso = match(c(min(AICc_lasso)),AICc_lasso)##Which index is this?\n",
    "print(paste(\"Lambda from AIC Lasso = \",fit.lasso$lambda[best_AIC_lasso]))##Find this lambda\n",
    "\n",
    "##BIC\n",
    "BIC_lasso<-log(n)*k - tLL \n",
    "min(BIC_lasso)##Best BIC\n",
    "best_BIC_lasso = match(c(min(BIC_lasso)),BIC_lasso)##Which index is this?\n",
    "print(paste(\"Lambda from BIC Lasso= \",fit.lasso$lambda[best_BIC_lasso]))\n",
    "\n",
    "\n",
    "yLassoAIC <- predict(fit10, s=fit.lasso$lambda[best_AIC_lasso], newx=x.test)\n",
    "mseLassoAIC <- mean((y.test - yLassoAIC)^2)\n",
    "print(paste(\"mse Lasso AIC = \",mseLassoAIC)) ##1.27423633930938 Best model so far\n",
    "\n",
    "yLassoBIC <- predict(fit10, s=fit.lasso$lambda[best_BIC_lasso], newx=x.test)\n",
    "mseLassoBIC <- mean((y.test - yLassoBIC)^2)\n",
    "print(paste(\"mse Lasso BIC = \",mseLassoBIC)) ##1.31752470484621 Still better but not best\n",
    "\n",
    "##Elastic Net\n",
    "##AIC\n",
    "tLL <- fit.elnet$nulldev - deviance(fit.elnet)\n",
    "k <- fit.elnet$df\n",
    "n <- fit.elnet$nobs\n",
    "AICc_elnet <- -tLL+2*k+2*k*(k+1)/(n-k-1)\n",
    "min(AICc_elnet)##Best AIC\n",
    "best_AIC_elnet = match(c(min(AICc_elnet)),AICc_elnet)##Which index is this?\n",
    "print(paste(\"Lambda AIC enet =\",fit.elnet$lambda[best_AIC_elnet]))##Find this lambda\n",
    "\n",
    "##BIC\n",
    "BIC_elnet<-log(n)*k - tLL \n",
    "min(BIC_elnet)##Best BIC\n",
    "best_BIC_elnet = match(c(min(BIC_elnet)),BIC_elnet)##Which index is this?\n",
    "fit.elnet$lambda[best_BIC_elnet]\n",
    "\n",
    "yelnetAIC <- predict(fit5, s=fit.elnet$lambda[best_BIC_elnet], newx=x.test)\n",
    "mseelnetAIC <- mean((y.test - yelnetAIC)^2)\n",
    "print(paste(\"mse elnet = \",mseelnetAIC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO is the winner! LASSO is good at picking up a small signal through lots of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the folowing tutorial, the authors demonstrate the use of lasso in the logistic regresion. \n",
    "\n",
    "In this cases we will be transforming the continuous outcome to discrete, and the evaluation to the model will rely on the constrast differences between the categorical predictors. \n",
    "\n",
    "https://eight2late.wordpress.com/2017/07/11/a-gentle-introduction-to-logistic-regression-and-lasso-regularisation-using-r/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
