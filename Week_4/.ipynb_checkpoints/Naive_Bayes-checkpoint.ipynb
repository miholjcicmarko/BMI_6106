{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "### The first machine learning alorithm that we will review in this class is Naive Bayes that comes directly from the derivation of the Bayes Theorem that we saw in class.\n",
    "\n",
    "#### As we saw while evaluating the Bayes theorem, we can calculate a posterior conditional probability using prior information. \n",
    "\n",
    "## Recall\n",
    "\n",
    "![title](Bayes_theorem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using this approach we can evaluate the posterior probability of a particular model (hypothesis) given the data (evidence) by evaluating the likelihood that some prior data is observed given this hypothesis, the model itself divided by the evidence.\n",
    "\n",
    "![title](bayes_hypothesis.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's extend the bayes theorem to include multiple evidence, for example if we want to evaluate a model or class based on multiple evidence (dependent variables).\n",
    "\n",
    "$P(\\theta|\\textbf{D}) = P(\\theta ) \\frac{P(\\textbf{D} |\\theta)}{P(\\textbf{D})} ~~~~~|| I,$\n",
    "\n",
    "or \n",
    "\n",
    "$P(x|y) = P(x) \\frac{P(y|x)}{P(y)} ~~~~~|| I,$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### where $\\theta$ is the hypothesis or model and D is the data or evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Now if we have multiple independent lines of evidence, then we can rewrite the bayes theorem as\n",
    "\n",
    "# $P(x|y_{1},y_{2}...y_{n}) = \\frac{P(y_{1},y_{2}...y_{n}|x)*P(x)}{P(y_{1},y_{2}...y_{n})}$\n",
    "\n",
    "### Computing $P(y_{1},y_{2}...y_{n}|x)*P(x)$ \n",
    "\n",
    "$=P(y_{1},y_{2},,,y_{n},x)*P(y_{2},y_{3},,,y_{n},x)$\n",
    "\n",
    "$=P(y_{1}|y_{2},,,y_{n},x)*P(y_{2}|y_{3},,,y_{n},x)*P(y_{3},y_{4},,,y_{n},x)$\n",
    "\n",
    "=...\n",
    "\n",
    "$=P(y_{1}|y_{2},,,y_{n},x)*P(y_{2}|y_{3},,,y_{n},x)...*P(y_{n-1}|y_{n},x)*P(y_{n}|x)*P(x)$ \n",
    "\n",
    "### However, recall that all X are independent from each other, thus we can simply restate the equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $P(x|y_{1},y_{2}...y_{n}) \\infty \\prod_{i=1}^{n}p\\left(x_{i}|y\\right) * P(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes’ theorem can also be written neatly in terms of a likelihood ratio and odds O as\n",
    "\n",
    "$O(A|B) = O(A) · Λ(A|B)$\n",
    "\n",
    "where $O(A|B) = \\frac{P (A|B)}{P (A^C |B)}$\n",
    "are the odds of A given B,\n",
    "and \n",
    "\n",
    "$O(A) = \\frac{P (A)}{ P (A^{C})}$ are the odds of A by itself,\n",
    "\n",
    "while $Λ(A|B) = \\frac{L(A|B)}{L(A^C |B)} = \\frac{P (B|A)}{P (B|A^C)} $is the likelihood ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
